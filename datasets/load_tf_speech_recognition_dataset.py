#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Nov 22 17:27:42 2018

@author: malrawi

TensorFlow Speech Recognition dataset

Version 1: Has 64727 audio samples
https://www.kaggle.com/c/tensorflow-speech-recognition-challenge
(we used Kaggle train set)
http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz". 
BibTeX @article{speechcommands, title={Speech Commands: 
    A public dataset for single-word speech recognition.}, author={Warden, Pete}, 
    journal={Dataset available from http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz}, year={2017} }

    stored in /My Programs/all_data/tf_speech_recognition
    

Vresion 2: 
https://www.tensorflow.org/tutorials/sequences/audio_recognition
(2018) Speech commands dataset version 2. [Online]. Available:
http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz
Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition at 
https://arxiv.org/abs/1804.03209




"""
import os
import warnings
import numpy as np
from PIL import Image
from torch.utils.data import Dataset
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
from scipy import signal
from scipy.io import wavfile


# matplotlib inline
warnings.filterwarnings("ignore")

# folder_of_data         = '/home/malrawi/Desktop/My Programs/all_data/'
# tf_speech_recognition_data = 'tf_speech_recognition/train/audio/'
# audio_path = folder_of_data + tf_speech_recognition_data 
# test_pict_Path  = folder_of_data + tf_speech_recognition_data + 'images'


def log_specgram(audio, sample_rate, window_size=20,
                 step_size=10, eps=1e-10):
    
    nperseg = int(round(window_size * sample_rate / 1e3))
    noverlap = int(round(step_size * sample_rate / 1e3))
    freqs, _, spec = signal.spectrogram(audio,
                                    fs=sample_rate,
                                    window='hann',
                                    nperseg=nperseg,
                                    noverlap=noverlap,
                                    detrend=False)
    return freqs, np.log(spec.T.astype(np.float32) + eps)
   

     
def wav2img(wav_path):

    """        takes in wave file path       """
    
    # use soundfile library to read in the wave files
    samplerate, test_sound  = wavfile.read(wav_path)
    _, spectrogram = log_specgram(test_sound, samplerate)
       
    return spectrogram
    
  


def get_file_names(audio_path):
    # #### Identify all the subdirectories in the training directory
    word_names = []   # word_names = subFolderList
    for x in os.listdir(audio_path):
        if os.path.isdir(audio_path + '/' + x):
            word_names.append(x)
                              
   # sample_audio = []    
    all_files = [] 
    all_words  = []
    for a_word_name in word_names:    
        # get all the wave files
        word_files = [y for y in os.listdir(audio_path + a_word_name) if '.wav' in y]
        all_files = all_files + word_files
        all_words =  all_words + [a_word_name]*len(word_files)
        # collect the first file from each dir
        # sample_audio.append(audio_path  + a_word_name + '/'+ word_files[0])
        
        
    return all_files, all_words #, sample_audio, word_names
 
    

class TfSpeechDataset(Dataset):

    def __init__(self, cf, train=True, transform=None, 
                 data_idx= np.arange(1), complement_idx=False):
        """
        Args:
            
            transform (callable, optional): Optional transform to be applied
            on a sample.
            data_idx: numpy.ndarray as a vector ([1, 4, 5,...])  containing the idx
            used to select the set, if none is presented, idx's will be generated 
            randomly according to split_percentage. To generate the testing set, 
            the data_idx generated by the train_set should be passed to the class 
            constructor of the test_set. 
            complement_idx: generate the set from the complement of data_idx
        """
        self.cf = cf
        self.folder_of_data = cf.dataset_path_TF_SPEECH        
        self.train = train  # training set or test set
        self.transform = transform
        self.file_name = []
        self.words = []
                
        all_files, all_words = get_file_names(self.folder_of_data)
        len_data = len(all_files)
        
        if len(data_idx) == 1:  # this is safe as the lowest is one, when nothing is passed
            np.random.seed(cf.rnd_seed_value)
            data_idx = np.sort(np.random.choice(len_data, 
                                                size=int(len_data * cf.split_percentage_TFSPCH), replace=False) )
            
        if complement_idx:
            all_idx = np.arange(0, len_data)
            data_idx = np.sort( np.setdiff1d(all_idx, data_idx, assume_unique=False) )

        for idx in data_idx:            
            self.file_name.append(all_files[idx])
            self.words.append(all_words[idx])
        
        self.data_idx = data_idx
        self.weights = np.ones( len(data_idx), dtype = 'uint8' )
    
    def add_weights_of_words(self): # weights to balance the loss, if the data is unbalanced   
        N = len(self.word_str)
        wordfreq = [self.word_str.count(w) for w in self.word_str]
        weights = 1 - np.array(wordfreq, dtype = 'float32')/N        
        self.weights = weights
    

    def num_classes(self):          
       return len(self.cf.PHOC('abcd', self.cf)) # pasing 'dump' word to get the length
        
    ''' doubt doubt doubt '''    
    def __len__(self):
        return len(self.data_idx)
     
        
    
        
    def __getitem__(self, idx):
        img = wav2img(self.folder_of_data + self.words[idx] + '/' + self.file_name[idx])
        
        plt.imshow(img.T, aspect='auto', origin='lower')
        img = img.reshape(img.shape[0], img.shape[1], 1)
        ToPIL = transforms.ToPILImage() 
        data = ToPIL(img)
        if not(self.cf.TFSPCH_ifn_scale ==0): # resizing just the height            
            new_w = int(data.size[0]*self.cf.H_ifn_scale/data.size[1])
            if new_w>self.cf.MAX_IMAGE_WIDTH: 
                new_w = self.cf.MAX_IMAGE_WIDTH
            data = data.resize( (new_w, self.cf.H_ifn_scale), Image.ANTIALIAS)
                        
        # data = data.convert('RGB')  # needs to be done to have all datasets in the same mode
        
        word_str = self.words[idx]
        if self.transform:
            data = self.transform(data)
        
        target = self.cf.PHOC(word_str, self.cf)

        return data, target, word_str, self.weights[idx]
    

